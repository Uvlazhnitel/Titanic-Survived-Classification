{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Find project root = directory that contains \"src\"\n",
    "cwd = Path.cwd()\n",
    "root = cwd\n",
    "while root != root.parent and not (root / \"src\").exists():\n",
    "    root = root.parent\n",
    "\n",
    "if not (root / \"src\").exists():\n",
    "    raise RuntimeError(f\"Could not find 'src' directory starting from {cwd}\")\n",
    "\n",
    "# Add project root to sys.path (NOT src itself)\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "\n",
    "print(\"CWD:\", cwd)\n",
    "print(\"PROJECT_ROOT:\", root)\n",
    "print(\"Has src?:\", (root / \"src\").exists())\n",
    "print(\"Last sys.path entries:\", sys.path[:5])\n",
    "\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TARGET = \"Survived\" \n",
    "df_raw = pd.read_csv(root / 'data/raw/Titanic-Dataset.csv')\n",
    "X = df_raw.drop(columns=[TARGET])\n",
    "y = df_raw[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets with stratification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns\n",
    "num_cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Pclass\"]\n",
    "cat_cols = [\"Sex\", \"Embarked\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from src.preprocessing import build_preprocessing_hgb_native\n",
    "\n",
    "# Build preprocessing pipeline and get categorical indices\n",
    "preprocessing, cat_idx = build_preprocessing_hgb_native(num_cols, cat_cols)\n",
    "\n",
    "# Define the final model with optimized hyperparameters\n",
    "hgb_final = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.05,\n",
    "    max_iter=150, \n",
    "    max_leaf_nodes=30,\n",
    "    min_samples_leaf=21,\n",
    "    categorical_features=cat_idx,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create the final pipeline\n",
    "pipe_final = Pipeline([(\"preprocess\", preprocessing), (\"model\", hgb_final)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oof = pd.read_csv(\"../reports/train_oof_leader.csv\")\n",
    "\n",
    "y_train = df_oof[\"y_true\"].to_numpy()\n",
    "p_final = df_oof[\"p_pred\"].to_numpy()\n",
    "\n",
    "print(y_train.shape, p_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "pr_auc = round(average_precision_score(y_train, p_final), 4)\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = round(roc_auc_score(y_train, p_final), 4)\n",
    "\n",
    "print(f\"PR-AUC (AP): {pr_auc}\")\n",
    "print(f\"ROC-AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# define thresholds grid\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for t in thresholds:\n",
    "    # binary predictions at threshold t\n",
    "    y_pred = (p_final >= t).astype(int)\n",
    "    \n",
    "    # compute metrics at this threshold\n",
    "    precision = precision_score(y_train, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_train, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_train, y_pred, zero_division=0)\n",
    "    \n",
    "    rows.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    })\n",
    "\n",
    "df_thr = pd.DataFrame(rows)\n",
    "\n",
    "# ensure reports folder exists\n",
    "Path(\"../reports\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save metrics table\n",
    "df_thr.to_csv(\"../reports/threshold_metrics_oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Precision vs threshold\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"precision\"])\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision vs Threshold (OOF)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/figures/precision_vs_threshold_oof_leader.png\", dpi=150)\n",
    "\n",
    "# Recall vs threshold\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"recall\"])\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall vs Threshold (OOF)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/figures/recall_vs_threshold_oof_leader.png\", dpi=150)\n",
    "\n",
    "# F1 vs threshold\n",
    "plt.figure()\n",
    "plt.plot(df_thr[\"threshold\"], df_thr[\"f1\"])\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"F1 vs Threshold (OOF)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../reports/figures/f1_vs_threshold_oof_leader.png\", dpi=150)\n",
    "\n",
    "# PR OOF curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "FIGURES_DIR = Path(\"../reports/figures\")\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "plt.figure()\n",
    "RocCurveDisplay.from_predictions(y_train, p_final)\n",
    "plt.title(\"ROC curve (train OOF, final leader)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"roc_oof_leader.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "PrecisionRecallDisplay.from_predictions(y_train, p_final)\n",
    "plt.title(\"Precision-Recall curve (train OOF, final leader)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"pr_oof_leader.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, p_final)\n",
    "TARGET_PRECISION = 0.85\n",
    "\n",
    "# Find indices where precision >= target (exclude i=0 since it has no corresponding threshold)\n",
    "mask = (precision >= TARGET_PRECISION)\n",
    "cand_idx = np.where(mask)[0][1:]  # Exclude i=0\n",
    "\n",
    "if cand_idx.size > 0:\n",
    "    # Pick the candidate with max recall among those meeting precision target\n",
    "    chosen_idx = cand_idx[np.argmax(recall[cand_idx])]\n",
    "    chosen_thr = thresholds[chosen_idx - 1]  # Map i -> thresholds[i-1]\n",
    "    strategy = f\"precision≥{TARGET_PRECISION:.2f} → max recall\"\n",
    "else:\n",
    "    # Fallback: choose threshold that maximizes F1 (ignore i=0, which has no corresponding threshold).\n",
    "    # valid contains indices starting from 1, so chosen_idx is >=1. thresholds is indexed by (chosen_idx - 1).\n",
    "    f1_curve = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
    "    valid = np.arange(1, len(precision))  # Ignore i=0\n",
    "    chosen_idx = valid[np.nanargmax(f1_curve[valid])]\n",
    "    chosen_thr = thresholds[chosen_idx - 1]\n",
    "    strategy = f\"max F1 (target precision {TARGET_PRECISION:.2f} unattainable on OOF)\"\n",
    "\n",
    "# Print strategy and chosen threshold\n",
    "print(\"Strategy:\", strategy)\n",
    "print(\"Chosen index:\", chosen_idx)\n",
    "print(\"Chosen threshold:\", round(chosen_thr, 3))\n",
    "print(\"Point on PR: precision=\", round(precision[chosen_idx], 3),\n",
    "      \"recall=\", round(recall[chosen_idx], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../reports\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.save(\"../reports/threshold_final.npy\", np.array([chosen_thr], dtype=float))\n",
    "\n",
    "thr_loaded = float(np.load(\"../reports/threshold_final.npy\")[0])\n",
    "print(f\"Saved Final threshold: {thr_loaded:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Apply the chosen threshold to classify predictions\n",
    "y_pred = (p_final >= chosen_thr).astype(int)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_train, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load final threshold\n",
    "t_final = thr_loaded  \n",
    "\n",
    "# Define thresholds t1, t2, t3\n",
    "t1 = max(0, t_final - 0.02)  # Ensure t1 is not less than 0\n",
    "t2 = t_final\n",
    "t3 = min(1, t_final + 0.02)  # Ensure t3 is not greater than 1\n",
    "\n",
    "\n",
    "def calculate_metrics(threshold):\n",
    "    y_pred = (p_final >= threshold).astype(int)  # Convert scores to binary predictions\n",
    "    precision = precision_score(y_train, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_train, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_train, y_pred, zero_division=0)\n",
    "    return precision, recall, f1\n",
    "\n",
    "metrics_t1 = calculate_metrics(t1)\n",
    "metrics_t2 = calculate_metrics(t2)\n",
    "metrics_t3 = calculate_metrics(t3)\n",
    "\n",
    "# Print results\n",
    "print(f\"Threshold t1 = {t1:.2f}: Precision = {metrics_t1[0]:.3f}, Recall = {metrics_t1[1]:.3f}, F1 = {metrics_t1[2]:.3f}\")\n",
    "print(f\"Threshold t2 = {t2:.2f}: Precision = {metrics_t2[0]:.3f}, Recall = {metrics_t2[1]:.3f}, F1 = {metrics_t2[2]:.3f}\")\n",
    "print(f\"Threshold t3 = {t3:.2f}: Precision = {metrics_t3[0]:.3f}, Recall = {metrics_t3[1]:.3f}, F1 = {metrics_t3[2]:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
