{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).resolve().parents[1] if \"__file__\" in globals() else Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))           \n",
    "sys.path.append(str(PROJECT_ROOT / \"src\"))   \n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"PYTHONPATH patched:\", sys.path[-2:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "TARGET = \"Survived\" \n",
    "df_raw = pd.read_csv('../data/raw/Titanic-Dataset.csv')\n",
    "X = df_raw.drop(columns=[TARGET])\n",
    "y = df_raw[TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "cat_cols = [\"Sex\", \"Pclass\", \"Embarked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import build_preprocessing\n",
    "\n",
    "preprocessing = build_preprocessing(num_cols, cat_cols, remainder=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Build a full pipeline with preprocessing and model\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessing),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit the full pipeline\n",
    "full_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define Stratified K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Get cross-validated predictions\n",
    "y_pred = cross_val_predict(full_pipeline, X_train, y_train, cv=skf)\n",
    "\n",
    "# Check if the model has a decision function\n",
    "hasattr(full_pipeline, \"decision_function\") #True\n",
    "hasattr(full_pipeline, \"predict_proba\") #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cross-validated decision function scores\n",
    "oof_proba = cross_val_predict(full_pipeline, X_train, y_train, cv=skf, method=\"predict_proba\")[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "prec, rec, f1 = precision_score(y_train, y_pred), recall_score(y_train, y_pred), f1_score(y_train, y_pred)\n",
    "print(f\"Precision: {prec:.4f}, Recall: {rec:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We optimize precision for the \"survived\" class because false positive cases (FP) lead to a misallocation of resources/priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Assuming y_train and oof_proba are already defined\n",
    "precision, recall, _ = precision_recall_curve(y_train, oof_proba)\n",
    "\n",
    "# Calculate PR-AUC\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plot Precision-Recall Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f\"PR Curve (AUC = {pr_auc:.2f})\", color=\"blue\")\n",
    "plt.fill_between(recall, precision, alpha=0.2, color=\"blue\")  # Shade the area under the curve\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# Assuming y_train and oof_proba are already defined\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, oof_proba)\n",
    "\n",
    "# Calculate ROC-AUC\n",
    "roc_auc = roc_auc_score(y_train, oof_proba)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color=\"blue\", linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random Guess\")  # Diagonal line\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Build PR curve points from OOF probabilities\n",
    "precision, recall, thresholds = precision_recall_curve(y_train, oof_proba)\n",
    "thr_ext = np.r_[0.0, thresholds] # Extend thresholds to match precision and recall lengths\n",
    "print(len(precision), len(recall), len(thr_ext))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Given: precision, recall, thresholds from precision_recall_curve\n",
    "TARGET_PRECISION = 0.85\n",
    "\n",
    "# Find indices where precision >= target (exclude i=0 since it has no corresponding threshold)\n",
    "mask = (precision >= TARGET_PRECISION)\n",
    "cand_idx = np.where(mask)[0][1:]  # Exclude i=0\n",
    "\n",
    "if cand_idx.size > 0:\n",
    "    # Pick the candidate with max recall among those meeting precision target\n",
    "    chosen_idx = cand_idx[np.argmax(recall[cand_idx])]\n",
    "    chosen_thr = thresholds[chosen_idx - 1]  # Map i -> thresholds[i-1]\n",
    "    strategy = f\"precision≥{TARGET_PRECISION:.2f} → max recall\"\n",
    "else:\n",
    "    # Fallback: choose threshold that maximizes F1 (ignore i=0)\n",
    "    f1_curve = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
    "    valid = np.arange(1, len(precision))  # Ignore i=0\n",
    "    chosen_idx = valid[np.nanargmax(f1_curve[valid])]\n",
    "    chosen_thr = thresholds[chosen_idx - 1]\n",
    "    strategy = f\"max F1 (target precision {TARGET_PRECISION:.2f} unattainable on OOF)\"\n",
    "\n",
    "# Print strategy and chosen threshold\n",
    "print(\"Strategy:\", strategy)\n",
    "print(\"Chosen index:\", chosen_idx)\n",
    "print(\"Chosen threshold:\", round(chosen_thr, 3))\n",
    "print(\"Point on PR: precision=\", round(precision[chosen_idx], 3),\n",
    "      \"recall=\", round(recall[chosen_idx], 3))\n",
    "\n",
    "# Verify recomputed metrics on the same OOF scores\n",
    "y_hat = (oof_proba >= chosen_thr).astype(int)\n",
    "print(\"Recomputed on OOF: \",\n",
    "      \"precision=\", round(precision_score(y_train, y_hat), 6),\n",
    "      \"recall=\", round(recall_score(y_train, y_hat), 6),\n",
    "      \"f1=\", round(f1_score(y_train, y_hat), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, average_precision_score, roc_auc_score\n",
    "\n",
    "# Save the chosen threshold\n",
    "Path(\"../reports\").mkdir(parents=True, exist_ok=True)\n",
    "np.save(\"../reports/threshold.npy\", np.array([chosen_thr]))\n",
    "\n",
    "# Generate OOF predictions using the chosen threshold\n",
    "oof_pred = (oof_proba >= chosen_thr).astype(int)\n",
    "\n",
    "# Compute evaluation metrics at the chosen threshold\n",
    "cm = confusion_matrix(y_train, oof_pred)\n",
    "prec_at = precision_score(y_train, oof_pred, zero_division=0)\n",
    "rec_at  = recall_score(y_train, oof_pred, zero_division=0)\n",
    "f1_at   = f1_score(y_train, oof_pred, zero_division=0)\n",
    "\n",
    "# Compute AUC metrics\n",
    "ap_oof  = average_precision_score(y_train, oof_proba)  # PR-AUC (AP)\n",
    "roc_oof = roc_auc_score(y_train,oof_proba)            # ROC-AUC\n",
    "\n",
    "print(\"Confusion matrix @thr:\\n\", cm)\n",
    "print(f\"OOF @thr -> Precision={prec_at:.3f} | Recall={rec_at:.3f} | F1={f1_at:.3f}\")\n",
    "print(f\"OOF AUCs -> PR-AUC(AP)={ap_oof:.3f} | ROC-AUC={roc_oof:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & imports (run once) ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "\n",
    "# Consistent seaborn style\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Confusion matrices: baseline @0.50 and chosen @t* (raw and normalized) ===\n",
    "\n",
    "thr_baseline = 0.50\n",
    "\n",
    "# Predictions @ thresholds\n",
    "oof_pred_050 = (oof_proba >= thr_baseline).astype(int)\n",
    "oof_pred_thr = (oof_proba >= chosen_thr).astype(int)\n",
    "\n",
    "# Raw confusion matrices\n",
    "cm_050 = confusion_matrix(y_train, oof_pred_050)\n",
    "cm_thr = confusion_matrix(y_train, oof_pred_thr)\n",
    "\n",
    "# Normalized by true class (rows sum to 1)\n",
    "cm_050_norm = confusion_matrix(y_train, oof_pred_050, normalize=\"true\")\n",
    "cm_thr_norm = confusion_matrix(y_train, oof_pred_thr, normalize=\"true\")\n",
    "\n",
    "# Display raw (counts)\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "ax = sns.heatmap(\n",
    "    cm_050, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"True 0\", \"True 1\"],\n",
    "    annot_kws={\"fontsize\": 11}\n",
    ")\n",
    "ax.set_title(f\"Confusion Matrix OOF @0.50\")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "ax = sns.heatmap(\n",
    "    cm_thr, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"True 0\", \"True 1\"],\n",
    "    annot_kws={\"fontsize\": 11}\n",
    ")\n",
    "ax.set_title(f\"Confusion Matrix OOF @{chosen_thr:.3f}\")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display normalized (rates)\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "ax = sns.heatmap(\n",
    "    cm_050_norm, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False,\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"True 0\", \"True 1\"],\n",
    "    annot_kws={\"fontsize\": 11}\n",
    ")\n",
    "ax.set_title(f\"Confusion Matrix OOF @0.50 (normalized)\")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "ax = sns.heatmap(\n",
    "    cm_thr_norm, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False,\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"True 0\", \"True 1\"],\n",
    "    annot_kws={\"fontsize\": 11}\n",
    ")\n",
    "ax.set_title(f\"Confusion Matrix OOF @{chosen_thr:.3f} (normalized)\")\n",
    "ax.set_xlabel(\"Predicted label\")\n",
    "ax.set_ylabel(\"True label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2) PR curve with markers for 0.50 and chosen threshold ===\n",
    "# This creates a copy of your PR curve but with points highlighted at both thresholds.\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_train, oof_proba)\n",
    "thr_ext = np.r_[0.0, thr]  # align sizes: len(prec)==len(rec)==len(thr_ext)\n",
    "\n",
    "# Helper: nearest index on the curve to a given threshold value\n",
    "def nearest_idx_to_threshold(threshold_value, thr_extended):\n",
    "    return int(np.argmin(np.abs(thr_extended - threshold_value)))\n",
    "\n",
    "idx_050 = nearest_idx_to_threshold(thr_baseline, thr_ext)\n",
    "idx_thr = nearest_idx_to_threshold(chosen_thr, thr_ext)\n",
    "\n",
    "plt.figure(figsize=(6, 4.5))\n",
    "plt.plot(rec, prec, label=\"PR (OOF)\")\n",
    "\n",
    "# Mark baseline point\n",
    "plt.scatter(rec[idx_050], prec[idx_050], s=60, marker=\"o\", label=f\"@0.50  (P={prec[idx_050]:.2f}, R={rec[idx_050]:.2f})\")\n",
    "\n",
    "# Mark chosen threshold point\n",
    "plt.scatter(rec[idx_thr], prec[idx_thr], s=70, marker=\"s\", label=f\"@{chosen_thr:.3f} (P={prec[idx_thr]:.2f}, R={rec[idx_thr]:.2f})\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"PR curve (OOF) with threshold markers\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
